{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "import nshutils as nu\n",
    "import rich\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:nshconfig._src.config:Default hash function set for class: Invalid\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'nshconfig._src.invalid.Invalid'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: RootConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'nshconfig._src.root.RootConfig'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:matplotlib:matplotlib data path: /net/csefiles/coc-fung-cluster/lingyu/miniconda3/envs/mace-tune/lib/python3.10/site-packages/matplotlib/mpl-data\n",
      "DEBUG:matplotlib:CONFIGDIR=/nethome/lkong88/.config/matplotlib\n",
      "DEBUG:matplotlib:interactive is False\n",
      "DEBUG:matplotlib:platform is linux\n",
      "DEBUG:matplotlib:CACHEDIR=/nethome/lkong88/.cache/matplotlib\n",
      "DEBUG:matplotlib.font_manager:Using fontManager instance from /nethome/lkong88/.cache/matplotlib/fontlist-v390.json\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: MAELossConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.finetune.loss.MAELossConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: MSELossConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.finetune.loss.MSELossConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: HuberLossConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.finetune.loss.HuberLossConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: L2MAELossConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.finetune.loss.L2MAELossConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: PropertyConfigBase\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.finetune.properties.PropertyConfigBase'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: GraphPropertyConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.finetune.properties.GraphPropertyConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: EnergyPropertyConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.finetune.properties.EnergyPropertyConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: ForcesPropertyConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.finetune.properties.ForcesPropertyConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: StressesPropertyConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.finetune.properties.StressesPropertyConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: NormalizerConfigBase\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.normalization.NormalizerConfigBase'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: PerAtomNormalizerConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.normalization.PerAtomNormalizerConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: MeanStdNormalizerConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.normalization.MeanStdNormalizerConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: RMSNormalizerConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.normalization.RMSNormalizerConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: PerAtomReferencingNormalizerConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.normalization.PerAtomReferencingNormalizerConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: StepLRConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.finetune.lr_scheduler.StepLRConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: MultiStepLRConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.finetune.lr_scheduler.MultiStepLRConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: ExponentialConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.finetune.lr_scheduler.ExponentialConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: ReduceOnPlateauConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.finetune.lr_scheduler.ReduceOnPlateauConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: CosineAnnealingLRConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.finetune.lr_scheduler.CosineAnnealingLRConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: ConstantLRConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.finetune.lr_scheduler.ConstantLRConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: LinearLRConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.finetune.lr_scheduler.LinearLRConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: OptimizerConfigBase\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.finetune.optimizer.OptimizerConfigBase'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: AdamConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.finetune.optimizer.AdamConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: AdamWConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.finetune.optimizer.AdamWConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: SGDConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.finetune.optimizer.SGDConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: FinetuneModuleBaseConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.finetune.base.FinetuneModuleBaseConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: BaseFileConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'nshconfig_extra.file.base.BaseFileConfig'>\n",
      "DEBUG:google.cloud.storage._opentelemetry_tracing:This service is instrumented using OpenTelemetry. OpenTelemetry or one of its components could not be imported; please add compatible versions of opentelemetry-api and opentelemetry-instrumentation packages in order to get Storage Tracing data.\n",
      "/net/csefiles/coc-fung-cluster/lingyu/miniconda3/envs/mace-tune/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: CachedPathConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'nshconfig_extra.file.cached_path_.CachedPathConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: SSHConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'nshconfig_extra.file.ssh.SSHConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: RemoteSSHFileConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'nshconfig_extra.file.ssh.RemoteSSHFileConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: FAIRChemAtomsToGraphSystemConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.backbones.eqV2.model.FAIRChemAtomsToGraphSystemConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: EqV2BackboneConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.backbones.eqV2.model.EqV2BackboneConfig'>\n",
      "INFO:nshconfig._src.registry:Registered <class 'mattertune.backbones.eqV2.model.EqV2BackboneConfig'> with tag 'eqV2'.\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: CutoffsConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.backbones.jmp.model.CutoffsConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: MaxNeighborsConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.backbones.jmp.model.MaxNeighborsConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: JMPGraphComputerConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.backbones.jmp.model.JMPGraphComputerConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: JMPBackboneConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.backbones.jmp.model.JMPBackboneConfig'>\n",
      "INFO:nshconfig._src.registry:Registered <class 'mattertune.backbones.jmp.model.JMPBackboneConfig'> with tag 'jmp'.\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: M3GNetGraphComputerConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.backbones.m3gnet.model.M3GNetGraphComputerConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: M3GNetBackboneConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.backbones.m3gnet.model.M3GNetBackboneConfig'>\n",
      "INFO:nshconfig._src.registry:Registered <class 'mattertune.backbones.m3gnet.model.M3GNetBackboneConfig'> with tag 'm3gnet'.\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: MatterSimGraphConvertorConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.backbones.mattersim.model.MatterSimGraphConvertorConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: MatterSimBackboneConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.backbones.mattersim.model.MatterSimBackboneConfig'>\n",
      "INFO:nshconfig._src.registry:Registered <class 'mattertune.backbones.mattersim.model.MatterSimBackboneConfig'> with tag 'mattersim'.\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: ORBSystemConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.backbones.orb.model.ORBSystemConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: ORBBackboneConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.backbones.orb.model.ORBBackboneConfig'>\n",
      "INFO:nshconfig._src.registry:Registered <class 'mattertune.backbones.orb.model.ORBBackboneConfig'> with tag 'orb'.\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: MACEBackboneConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.backbones.mace_foundation.model.MACEBackboneConfig'>\n",
      "INFO:nshconfig._src.registry:Registered <class 'mattertune.backbones.mace_foundation.model.MACEBackboneConfig'> with tag 'mace'.\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: EarlyStoppingConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.callbacks.early_stopping.EarlyStoppingConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: EMAConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.callbacks.ema.EMAConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: ModelCheckpointConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.callbacks.model_checkpoint.ModelCheckpointConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: DatasetConfigBase\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.data.base.DatasetConfigBase'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: JSONDatasetConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.data.json_data.JSONDatasetConfig'>\n",
      "INFO:nshconfig._src.registry:Registered <class 'mattertune.data.json_data.JSONDatasetConfig'> with tag 'json'.\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: MatbenchDatasetConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.data.matbench.MatbenchDatasetConfig'>\n",
      "INFO:nshconfig._src.registry:Registered <class 'mattertune.data.matbench.MatbenchDatasetConfig'> with tag 'matbench'.\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: MPDatasetConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.data.mp.MPDatasetConfig'>\n",
      "INFO:nshconfig._src.registry:Registered <class 'mattertune.data.mp.MPDatasetConfig'> with tag 'mp'.\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: OMAT24DatasetConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.data.omat24.OMAT24DatasetConfig'>\n",
      "INFO:nshconfig._src.registry:Registered <class 'mattertune.data.omat24.OMAT24DatasetConfig'> with tag 'omat24'.\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: XYZDatasetConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.data.xyz.XYZDatasetConfig'>\n",
      "INFO:nshconfig._src.registry:Registered <class 'mattertune.data.xyz.XYZDatasetConfig'> with tag 'xyz'.\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: DataModuleBaseConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.data.datamodule.DataModuleBaseConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: ManualSplitDataModuleConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.data.datamodule.ManualSplitDataModuleConfig'>\n",
      "INFO:nshconfig._src.registry:Rebuilt <class 'mattertune.data.datamodule.ManualSplitDataModuleConfig'> schema due to registry changes.\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: AutoSplitDataModuleConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.data.datamodule.AutoSplitDataModuleConfig'>\n",
      "INFO:nshconfig._src.registry:Rebuilt <class 'mattertune.data.datamodule.AutoSplitDataModuleConfig'> schema due to registry changes.\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: CSVLoggerConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.loggers.CSVLoggerConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: WandbLoggerConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.loggers.WandbLoggerConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: TensorBoardLoggerConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.loggers.TensorBoardLoggerConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: RecipeConfigBase\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.recipes.base.RecipeConfigBase'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: EMARecipeConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.recipes.ema.EMARecipeConfig'>\n",
      "INFO:nshconfig._src.registry:Registered <class 'mattertune.recipes.ema.EMARecipeConfig'> with tag 'ema'.\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: PeftConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.recipes.lora.PeftConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: LoraConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.recipes.lora.LoraConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: LoRARecipeConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.recipes.lora.LoRARecipeConfig'>\n",
      "INFO:nshconfig._src.registry:Registered <class 'mattertune.recipes.lora.LoRARecipeConfig'> with tag 'lora'.\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: NoOpRecipeConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.recipes.noop.NoOpRecipeConfig'>\n",
      "INFO:nshconfig._src.registry:Registered <class 'mattertune.recipes.noop.NoOpRecipeConfig'> with tag 'no-op'.\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: TrainerConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.main.TrainerConfig'>\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: MatterTunerConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.main.MatterTunerConfig'>\n",
      "INFO:nshconfig._src.registry:Rebuilt <class 'mattertune.main.MatterTunerConfig'> schema due to registry changes.\n",
      "INFO:nshconfig._src.registry:Rebuilt <class 'mattertune.main.MatterTunerConfig'> schema due to registry changes.\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: AtomsListDatasetConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.data.atoms_list.AtomsListDatasetConfig'>\n",
      "INFO:nshconfig._src.registry:Registered <class 'mattertune.data.atoms_list.AtomsListDatasetConfig'> with tag 'atoms_list'.\n",
      "INFO:nshconfig._src.registry:Rebuilt <class 'mattertune.data.datamodule.ManualSplitDataModuleConfig'> schema due to registry changes.\n",
      "INFO:nshconfig._src.registry:Rebuilt <class 'mattertune.data.datamodule.AutoSplitDataModuleConfig'> schema due to registry changes.\n",
      "INFO:nshconfig._src.registry:Rebuilt <class 'mattertune.main.MatterTunerConfig'> schema due to registry changes.\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: DBDatasetConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.data.db.DBDatasetConfig'>\n",
      "INFO:nshconfig._src.registry:Registered <class 'mattertune.data.db.DBDatasetConfig'> with tag 'db'.\n",
      "INFO:nshconfig._src.registry:Rebuilt <class 'mattertune.data.datamodule.ManualSplitDataModuleConfig'> schema due to registry changes.\n",
      "INFO:nshconfig._src.registry:Rebuilt <class 'mattertune.data.datamodule.AutoSplitDataModuleConfig'> schema due to registry changes.\n",
      "INFO:nshconfig._src.registry:Rebuilt <class 'mattertune.main.MatterTunerConfig'> schema due to registry changes.\n",
      "DEBUG:nshconfig._src.config:Default hash function set for class: MPTrajDatasetConfig\n",
      "DEBUG:nshconfig._src.config:__pydantic_init_subclass__: <class 'mattertune.data.mptraj.MPTrajDatasetConfig'>\n",
      "INFO:nshconfig._src.registry:Registered <class 'mattertune.data.mptraj.MPTrajDatasetConfig'> with tag 'mptraj'.\n",
      "INFO:nshconfig._src.registry:Rebuilt <class 'mattertune.data.datamodule.ManualSplitDataModuleConfig'> schema due to registry changes.\n",
      "INFO:nshconfig._src.registry:Rebuilt <class 'mattertune.data.datamodule.AutoSplitDataModuleConfig'> schema due to registry changes.\n",
      "INFO:nshconfig._src.registry:Rebuilt <class 'mattertune.main.MatterTunerConfig'> schema due to registry changes.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MatterTunerConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">data</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AutoSplitDataModuleConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">batch_size</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">num_workers</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'auto'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">pin_memory</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">dataset</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">XYZDatasetConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'xyz'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">src</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">PosixPath</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'data/Perovskite.xyz'</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">down_sample</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">down_sample_refill</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">train_split</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">validation_split</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'auto'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">shuffle</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">shuffle_seed</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">42</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">model</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MACEBackboneConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">reset_backbone</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">freeze_backbone</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">reset_output_heads</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">use_pretrained_normalizers</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">properties</span>=<span style=\"font-weight: bold\">[</span>\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">EnergyPropertyConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'energy'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'float'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">loss</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MAELossConfig</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'mae'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reduction</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span><span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">loss_coefficient</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'energy'</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ForcesPropertyConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'forces'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'float'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">loss</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MAELossConfig</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'mae'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reduction</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span><span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">loss_coefficient</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'forces'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">conservative</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "            <span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">StressesPropertyConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'stresses'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'float'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">loss</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MAELossConfig</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'mae'</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reduction</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'mean'</span><span style=\"font-weight: bold\">)</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">loss_coefficient</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1.0</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">type</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'stresses'</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">conservative</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">]</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">optimizer</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">AdamWConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">per_parameter_hparams</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'AdamW'</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">lr</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0001</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">eps</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1e-08</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">betas</span>=<span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.9</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.999</span><span style=\"font-weight: bold\">)</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">weight_decay</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.01</span>,\n",
       "            <span style=\"color: #808000; text-decoration-color: #808000\">amsgrad</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">lr_scheduler</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">ignore_gpu_batch_transform_error</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">normalizers</span>=<span style=\"font-weight: bold\">{}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">name</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'mace'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">pretrained_model</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'mace-medium'</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">trainer</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">TrainerConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">accelerator</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'auto'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">strategy</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'auto'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">num_nodes</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">devices</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'auto'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">precision</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'32-true'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">deterministic</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">max_epochs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">min_epochs</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">max_steps</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">min_steps</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">max_time</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">val_check_interval</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">check_val_every_n_epoch</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">log_every_n_steps</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">gradient_clip_val</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">gradient_clip_algorithm</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">checkpoint</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">early_stopping</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">ema</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">loggers</span>=<span style=\"color: #008000; text-decoration-color: #008000\">'default'</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">additional_trainer_kwargs</span>=<span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'fast_dev_run'</span>: <span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">recipes</span>=<span style=\"font-weight: bold\">[]</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mMatterTunerConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mdata\u001b[0m=\u001b[1;35mAutoSplitDataModuleConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mbatch_size\u001b[0m=\u001b[1;36m2\u001b[0m,\n",
       "        \u001b[33mnum_workers\u001b[0m=\u001b[32m'auto'\u001b[0m,\n",
       "        \u001b[33mpin_memory\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "        \u001b[33mdataset\u001b[0m=\u001b[1;35mXYZDatasetConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mtype\u001b[0m=\u001b[32m'xyz'\u001b[0m,\n",
       "            \u001b[33msrc\u001b[0m=\u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'data/Perovskite.xyz'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[33mdown_sample\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mdown_sample_refill\u001b[0m=\u001b[3;91mFalse\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[33mtrain_split\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.8\u001b[0m,\n",
       "        \u001b[33mvalidation_split\u001b[0m=\u001b[32m'auto'\u001b[0m,\n",
       "        \u001b[33mshuffle\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "        \u001b[33mshuffle_seed\u001b[0m=\u001b[1;36m42\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mmodel\u001b[0m=\u001b[1;35mMACEBackboneConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mreset_backbone\u001b[0m=\u001b[3;91mFalse\u001b[0m,\n",
       "        \u001b[33mfreeze_backbone\u001b[0m=\u001b[3;91mFalse\u001b[0m,\n",
       "        \u001b[33mreset_output_heads\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "        \u001b[33muse_pretrained_normalizers\u001b[0m=\u001b[3;91mFalse\u001b[0m,\n",
       "        \u001b[33mproperties\u001b[0m=\u001b[1m[\u001b[0m\n",
       "            \u001b[1;35mEnergyPropertyConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mname\u001b[0m=\u001b[32m'energy'\u001b[0m,\n",
       "                \u001b[33mdtype\u001b[0m=\u001b[32m'float'\u001b[0m,\n",
       "                \u001b[33mloss\u001b[0m=\u001b[1;35mMAELossConfig\u001b[0m\u001b[1m(\u001b[0m\u001b[33mname\u001b[0m=\u001b[32m'mae'\u001b[0m, \u001b[33mreduction\u001b[0m=\u001b[32m'mean'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "                \u001b[33mloss_coefficient\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
       "                \u001b[33mtype\u001b[0m=\u001b[32m'energy'\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[1;35mForcesPropertyConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mname\u001b[0m=\u001b[32m'forces'\u001b[0m,\n",
       "                \u001b[33mdtype\u001b[0m=\u001b[32m'float'\u001b[0m,\n",
       "                \u001b[33mloss\u001b[0m=\u001b[1;35mMAELossConfig\u001b[0m\u001b[1m(\u001b[0m\u001b[33mname\u001b[0m=\u001b[32m'mae'\u001b[0m, \u001b[33mreduction\u001b[0m=\u001b[32m'mean'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "                \u001b[33mloss_coefficient\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
       "                \u001b[33mtype\u001b[0m=\u001b[32m'forces'\u001b[0m,\n",
       "                \u001b[33mconservative\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "            \u001b[1m)\u001b[0m,\n",
       "            \u001b[1;35mStressesPropertyConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mname\u001b[0m=\u001b[32m'stresses'\u001b[0m,\n",
       "                \u001b[33mdtype\u001b[0m=\u001b[32m'float'\u001b[0m,\n",
       "                \u001b[33mloss\u001b[0m=\u001b[1;35mMAELossConfig\u001b[0m\u001b[1m(\u001b[0m\u001b[33mname\u001b[0m=\u001b[32m'mae'\u001b[0m, \u001b[33mreduction\u001b[0m=\u001b[32m'mean'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "                \u001b[33mloss_coefficient\u001b[0m=\u001b[1;36m1\u001b[0m\u001b[1;36m.0\u001b[0m,\n",
       "                \u001b[33mtype\u001b[0m=\u001b[32m'stresses'\u001b[0m,\n",
       "                \u001b[33mconservative\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m]\u001b[0m,\n",
       "        \u001b[33moptimizer\u001b[0m=\u001b[1;35mAdamWConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "            \u001b[33mper_parameter_hparams\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "            \u001b[33mname\u001b[0m=\u001b[32m'AdamW'\u001b[0m,\n",
       "            \u001b[33mlr\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0001\u001b[0m,\n",
       "            \u001b[33meps\u001b[0m=\u001b[1;36m1e\u001b[0m\u001b[1;36m-08\u001b[0m,\n",
       "            \u001b[33mbetas\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m0.9\u001b[0m, \u001b[1;36m0.999\u001b[0m\u001b[1m)\u001b[0m,\n",
       "            \u001b[33mweight_decay\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.01\u001b[0m,\n",
       "            \u001b[33mamsgrad\u001b[0m=\u001b[3;91mFalse\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[33mlr_scheduler\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mignore_gpu_batch_transform_error\u001b[0m=\u001b[3;92mTrue\u001b[0m,\n",
       "        \u001b[33mnormalizers\u001b[0m=\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mname\u001b[0m=\u001b[32m'mace'\u001b[0m,\n",
       "        \u001b[33mpretrained_model\u001b[0m=\u001b[32m'mace-medium'\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mtrainer\u001b[0m=\u001b[1;35mTrainerConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33maccelerator\u001b[0m=\u001b[32m'auto'\u001b[0m,\n",
       "        \u001b[33mstrategy\u001b[0m=\u001b[32m'auto'\u001b[0m,\n",
       "        \u001b[33mnum_nodes\u001b[0m=\u001b[1;36m1\u001b[0m,\n",
       "        \u001b[33mdevices\u001b[0m=\u001b[32m'auto'\u001b[0m,\n",
       "        \u001b[33mprecision\u001b[0m=\u001b[32m'32-true'\u001b[0m,\n",
       "        \u001b[33mdeterministic\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mmax_epochs\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mmin_epochs\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mmax_steps\u001b[0m=\u001b[1;36m-1\u001b[0m,\n",
       "        \u001b[33mmin_steps\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mmax_time\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mval_check_interval\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mcheck_val_every_n_epoch\u001b[0m=\u001b[1;36m1\u001b[0m,\n",
       "        \u001b[33mlog_every_n_steps\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mgradient_clip_val\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mgradient_clip_algorithm\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mcheckpoint\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mearly_stopping\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mema\u001b[0m=\u001b[3;35mNone\u001b[0m,\n",
       "        \u001b[33mloggers\u001b[0m=\u001b[32m'default'\u001b[0m,\n",
       "        \u001b[33madditional_trainer_kwargs\u001b[0m=\u001b[1m{\u001b[0m\u001b[32m'fast_dev_run'\u001b[0m: \u001b[3;92mTrue\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[33mrecipes\u001b[0m=\u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/csefiles/coc-fung-cluster/lingyu/miniconda3/envs/mace-tune/lib/python3.10/site-packages/e3nn/o3/_wigner.py:10: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuequivariance or cuequivariance_torch is not available. Cuequivariance acceleration will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
      "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
      "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
      "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
      "/nethome/lkong88/mace/mace/calculators/mace.py:143: UserWarning: Environment variable TORCH_FORCE_NO_WEIGHTS_ONLY_LOAD detected, since the`weights_only` argument was not explicitly passed to `torch.load`, forcing weights_only=False.\n",
      "  torch.load(f=model_path, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Materials Project MACE for MACECalculator with /nethome/lkong88/.cache/mace/20231203mace128L1_epoch199model\n",
      "Using float32 for MACECalculator, which is faster but less accurate. Recommended for MD. Use float64 for geometry optimization.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:CUDA version: 12.6, CUDA device: 0\n",
      "INFO:mattertune.main:The model requires inference_mode to be disabled. Setting inference_mode=False.\n",
      "INFO: Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO: Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\n",
      "INFO: You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:lightning.pytorch.utilities.rank_zero:You are using a CUDA device ('NVIDIA A40') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "INFO:mattertune.data.xyz:Loaded 85 atoms from data/Perovskite.xyz\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO: \n",
      "  | Name          | Type            | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | backbone      | ScaleShiftMACE  | 4.7 M  | train\n",
      "1 | train_metrics | FinetuneMetrics | 0      | train\n",
      "2 | val_metrics   | FinetuneMetrics | 0      | train\n",
      "3 | test_metrics  | FinetuneMetrics | 0      | train\n",
      "4 | normalizers   | ModuleDict      | 0      | train\n",
      "----------------------------------------------------------\n",
      "4.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.7 M     Total params\n",
      "18.755    Total estimated model params size (MB)\n",
      "140       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "INFO:lightning.pytorch.callbacks.model_summary:\n",
      "  | Name          | Type            | Params | Mode \n",
      "----------------------------------------------------------\n",
      "0 | backbone      | ScaleShiftMACE  | 4.7 M  | train\n",
      "1 | train_metrics | FinetuneMetrics | 0      | train\n",
      "2 | val_metrics   | FinetuneMetrics | 0      | train\n",
      "3 | test_metrics  | FinetuneMetrics | 0      | train\n",
      "4 | normalizers   | ModuleDict      | 0      | train\n",
      "----------------------------------------------------------\n",
      "4.7 M     Trainable params\n",
      "0         Non-trainable params\n",
      "4.7 M     Total params\n",
      "18.755    Total estimated model params size (MB)\n",
      "140       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using head Default out of ['Default']\n",
      "Default dtype float32 does not match model dtype float64, converting models to float32.\n",
      "Epoch 0: 100%|| 1/1 [00:02<00:00,  0.40it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/csefiles/coc-fung-cluster/lingyu/miniconda3/envs/mace-tune/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:79: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 136. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
      "INFO: `Trainer.fit` stopped: `max_steps=1` reached.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer.fit` stopped: `max_steps=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|| 1/1 [00:02<00:00,  0.39it/s]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import mattertune.configs as MC\n",
    "from mattertune import MatterTuner\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "\n",
    "def hparams():\n",
    "    hparams = MC.MatterTunerConfig.draft()\n",
    "\n",
    "    # Model hparams \n",
    "    hparams.model = MC.MACEBackboneConfig.draft()\n",
    "    hparams.model.pretrained_model = \"mace-medium\"\n",
    "    hparams.model.ignore_gpu_batch_transform_error = True\n",
    "    hparams.model.optimizer = MC.AdamWConfig(lr=1.0e-4)\n",
    "\n",
    "    hparams.model.properties = []\n",
    "    energy = MC.EnergyPropertyConfig(loss=MC.MAELossConfig())\n",
    "    hparams.model.properties.append(energy)\n",
    "    forces = MC.ForcesPropertyConfig(loss=MC.MAELossConfig(), conservative=True)\n",
    "    hparams.model.properties.append(forces)\n",
    "    stresses = MC.StressesPropertyConfig(loss=MC.MAELossConfig(), conservative=True)\n",
    "    hparams.model.properties.append(stresses)\n",
    "\n",
    "    # Data hparams\n",
    "    hparams.data = MC.AutoSplitDataModuleConfig.draft()\n",
    "    hparams.data.dataset = MC.XYZDatasetConfig.draft()\n",
    "    hparams.data.dataset.src = Path(\"./data/Perovskite.xyz\")\n",
    "    hparams.data.train_split = 0.8\n",
    "    hparams.data.batch_size = 2\n",
    "\n",
    "    # Trainer hparams\n",
    "    hparams.trainer.additional_trainer_kwargs = {\"fast_dev_run\": True}\n",
    "\n",
    "    hparams = hparams.finalize()\n",
    "    rich.print(hparams)\n",
    "    return hparams\n",
    "\n",
    "\n",
    "hp = hparams()\n",
    "tune_output = MatterTuner(hp).tune()\n",
    "model, trainer = tune_output.model, tune_output.trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<mattertune.wrappers.property_predictor.MatterTunePropertyPredictor object at 0x7fc5580eb160>\n",
      "<mattertune.wrappers.ase_calculator.MatterTuneCalculator object at 0x7fc2fcf952a0>\n"
     ]
    }
   ],
   "source": [
    "property_predictor = model.property_predictor()\n",
    "print(property_predictor)\n",
    "\n",
    "calculator = model.ase_calculator()\n",
    "print(calculator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mattertune.wrappers.property_predictor:The model requires inference_mode to be disabled. Setting inference_mode=False.\n",
      "INFO: You are running in `Trainer(barebones=True)` mode. All features that may impact raw speed have been disabled to facilitate analyzing the Trainer overhead. Specifically, the following features are deactivated:\n",
      " - Checkpointing: `Trainer(enable_checkpointing=True)`\n",
      " - Progress bar: `Trainer(enable_progress_bar=True)`\n",
      " - Model summary: `Trainer(enable_model_summary=True)`\n",
      " - Logging: `Trainer(logger=True)`, `Trainer(log_every_n_steps>0)`, `LightningModule.log(...)`, `LightningModule.log_dict(...)`\n",
      " - Sanity checking: `Trainer(num_sanity_val_steps>0)`\n",
      " - Development run: `Trainer(fast_dev_run=True)`\n",
      " - Anomaly detection: `Trainer(detect_anomaly=True)`\n",
      " - Profiling: `Trainer(profiler=...)`\n",
      "INFO:lightning.pytorch.utilities.rank_zero:You are running in `Trainer(barebones=True)` mode. All features that may impact raw speed have been disabled to facilitate analyzing the Trainer overhead. Specifically, the following features are deactivated:\n",
      " - Checkpointing: `Trainer(enable_checkpointing=True)`\n",
      " - Progress bar: `Trainer(enable_progress_bar=True)`\n",
      " - Model summary: `Trainer(enable_model_summary=True)`\n",
      " - Logging: `Trainer(logger=True)`, `Trainer(log_every_n_steps>0)`, `LightningModule.log(...)`, `LightningModule.log_dict(...)`\n",
      " - Sanity checking: `Trainer(num_sanity_val_steps>0)`\n",
      " - Development run: `Trainer(fast_dev_run=True)`\n",
      " - Anomaly detection: `Trainer(detect_anomaly=True)`\n",
      " - Profiling: `Trainer(profiler=...)`\n",
      "INFO: Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: `Trainer(barebones=True)` started running. The progress bar is disabled so you might want to manually print the progress in your model.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer(barebones=True)` started running. The progress bar is disabled so you might want to manually print the progress in your model.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atoms(symbols='H2O', pbc=True, cell=[10.0, 10.0, 10.0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/net/csefiles/coc-fung-cluster/lingyu/miniconda3/envs/mace-tune/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'predict_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=63` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'energy': tensor(-10.8586), 'forces': tensor([[-0.0000, -0.2362, -2.2739],\n",
      "        [-0.0000,  3.2311, -0.9571],\n",
      "        [-0.0000, -2.9948,  3.2311]]), 'stresses': tensor([[ 0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0030, -0.0032],\n",
      "        [ 0.0000, -0.0032,  0.0010]])}]\n"
     ]
    }
   ],
   "source": [
    "import ase\n",
    "\n",
    "# Create a test periodic system\n",
    "atoms = ase.Atoms(\n",
    "    \"H2O\", positions=[[0, 0, 0], [0, 0, 1], [0, 1, 0]], cell=[10, 10, 10], pbc=True\n",
    ")\n",
    "print(atoms)\n",
    "\n",
    "print(property_predictor.predict([atoms], model.hparams.properties))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atoms(symbols='H2O', pbc=True, cell=[10.0, 10.0, 10.0])\n",
      "-10.858636856079102\n"
     ]
    }
   ],
   "source": [
    "import ase\n",
    "\n",
    "# Create a test periodic system\n",
    "atoms = ase.Atoms(\n",
    "    \"H2O\", positions=[[0, 0, 0], [0, 0, 1], [0, 1, 0]], cell=[10, 10, 10], pbc=True\n",
    ")\n",
    "print(atoms)\n",
    "\n",
    "# Set the calculator\n",
    "atoms.calc = calculator\n",
    "\n",
    "# Calculate the energy\n",
    "energy = atoms.get_potential_energy()\n",
    "print(energy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mattertune.wrappers.property_predictor:The model requires inference_mode to be disabled. Setting inference_mode=False.\n",
      "INFO: You are running in `Trainer(barebones=True)` mode. All features that may impact raw speed have been disabled to facilitate analyzing the Trainer overhead. Specifically, the following features are deactivated:\n",
      " - Checkpointing: `Trainer(enable_checkpointing=True)`\n",
      " - Progress bar: `Trainer(enable_progress_bar=True)`\n",
      " - Model summary: `Trainer(enable_model_summary=True)`\n",
      " - Logging: `Trainer(logger=True)`, `Trainer(log_every_n_steps>0)`, `LightningModule.log(...)`, `LightningModule.log_dict(...)`\n",
      " - Sanity checking: `Trainer(num_sanity_val_steps>0)`\n",
      " - Development run: `Trainer(fast_dev_run=True)`\n",
      " - Anomaly detection: `Trainer(detect_anomaly=True)`\n",
      " - Profiling: `Trainer(profiler=...)`\n",
      "INFO:lightning.pytorch.utilities.rank_zero:You are running in `Trainer(barebones=True)` mode. All features that may impact raw speed have been disabled to facilitate analyzing the Trainer overhead. Specifically, the following features are deactivated:\n",
      " - Checkpointing: `Trainer(enable_checkpointing=True)`\n",
      " - Progress bar: `Trainer(enable_progress_bar=True)`\n",
      " - Model summary: `Trainer(enable_model_summary=True)`\n",
      " - Logging: `Trainer(logger=True)`, `Trainer(log_every_n_steps>0)`, `LightningModule.log(...)`, `LightningModule.log_dict(...)`\n",
      " - Sanity checking: `Trainer(num_sanity_val_steps>0)`\n",
      " - Development run: `Trainer(fast_dev_run=True)`\n",
      " - Anomaly detection: `Trainer(detect_anomaly=True)`\n",
      " - Profiling: `Trainer(profiler=...)`\n",
      "INFO: Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:Trainer will use only 1 of 4 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=4)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "INFO: GPU available: True (cuda), used: True\n",
      "INFO:lightning.pytorch.utilities.rank_zero:GPU available: True (cuda), used: True\n",
      "INFO: TPU available: False, using: 0 TPU cores\n",
      "INFO:lightning.pytorch.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
      "INFO: HPU available: False, using: 0 HPUs\n",
      "INFO:lightning.pytorch.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
      "INFO: `Trainer(barebones=True)` started running. The progress bar is disabled so you might want to manually print the progress in your model.\n",
      "INFO:lightning.pytorch.utilities.rank_zero:`Trainer(barebones=True)` started running. The progress bar is disabled so you might want to manually print the progress in your model.\n",
      "INFO: LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "INFO:lightning.pytorch.accelerators.cuda:LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ase.Atoms 1 energy: tensor(-10.8586)\n",
      "ase.Atoms 1 forces: tensor([[-0.0000, -0.2362, -2.2739],\n",
      "        [-0.0000,  3.2311, -0.9571],\n",
      "        [-0.0000, -2.9948,  3.2311]])\n",
      "ase.Atoms 2 energy: tensor(-10.8586)\n",
      "ase.Atoms 2 forces: tensor([[-0.0000, -0.2362, -2.2739],\n",
      "        [-0.0000,  3.2311, -0.9571],\n",
      "        [-0.0000, -2.9948,  3.2311]])\n"
     ]
    }
   ],
   "source": [
    "property_predictor = model.property_predictor()\n",
    "atoms_1 = ase.Atoms(\n",
    "    \"H2O\", positions=[[0, 0, 0], [0, 0, 1], [0, 1, 0]], cell=[10, 10, 10], pbc=True\n",
    ")\n",
    "atoms_2 = ase.Atoms(\n",
    "    \"H2O\", positions=[[0, 0, 0], [0, 0, 1], [0, 1, 0]], cell=[10, 10, 10], pbc=True\n",
    ")\n",
    "atoms = [atoms_1, atoms_2]\n",
    "predictions = property_predictor.predict(atoms, [\"energy\", \"forces\"])\n",
    "print(\"ase.Atoms 1 energy:\", predictions[0][\"energy\"])\n",
    "print(\"ase.Atoms 1 forces:\", predictions[0][\"forces\"])\n",
    "print(\"ase.Atoms 2 energy:\", predictions[1][\"energy\"])\n",
    "print(\"ase.Atoms 2 forces:\", predictions[1][\"forces\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mattersim-tune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
