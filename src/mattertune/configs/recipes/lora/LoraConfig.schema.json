{
    "properties": {
        "alpha_pattern": {
            "default": {},
            "description": "Mapping of layer names/patterns to custom alphas different from default lora_alpha.",
            "title": "Alpha Pattern",
            "type": "object"
        },
        "bias": {
            "default": "none",
            "description": "Bias type for LoRA. Controls which biases are updated during training.",
            "enum": [
                "none",
                "all",
                "lora_only"
            ],
            "title": "Bias",
            "type": "string"
        },
        "fan_in_fan_out": {
            "default": false,
            "description": "Set True if target layer stores weights as (fan_in, fan_out).",
            "title": "Fan In Fan Out",
            "type": "boolean"
        },
        "inference_mode": {
            "default": false,
            "description": "Whether to use inference mode.",
            "title": "Inference Mode",
            "type": "boolean"
        },
        "init_lora_weights": {
            "anyOf": [
                {
                    "type": "boolean"
                },
                {
                    "const": "gaussian",
                    "enum": [
                        "gaussian"
                    ],
                    "type": "string"
                }
            ],
            "default": true,
            "description": "Initialization method for LoRA weights.",
            "title": "Init Lora Weights"
        },
        "layers_pattern": {
            "anyOf": [
                {
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                },
                {
                    "type": "string"
                },
                {
                    "type": "null"
                }
            ],
            "default": null,
            "description": "Layer pattern name used with layers_to_transform.",
            "title": "Layers Pattern"
        },
        "layers_to_transform": {
            "anyOf": [
                {
                    "items": {
                        "type": "integer"
                    },
                    "type": "array"
                },
                {
                    "type": "integer"
                },
                {
                    "type": "null"
                }
            ],
            "default": null,
            "description": "Specific layer indices to apply LoRA transformation to.",
            "title": "Layers To Transform"
        },
        "lora_alpha": {
            "default": 8,
            "description": "Alpha parameter for LoRA scaling.",
            "title": "Lora Alpha",
            "type": "integer"
        },
        "lora_dropout": {
            "default": 0.0,
            "description": "Dropout probability for LoRA layers.",
            "title": "Lora Dropout",
            "type": "number"
        },
        "modules_to_save": {
            "anyOf": [
                {
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                },
                {
                    "type": "null"
                }
            ],
            "default": null,
            "description": "Additional modules to be trained and saved besides LoRA layers.",
            "title": "Modules To Save"
        },
        "peft_type": {
            "anyOf": [
                {
                    "type": "string"
                },
                {
                    "type": "null"
                }
            ],
            "default": null,
            "description": "Type of PEFT method being used.",
            "title": "Peft Type"
        },
        "r": {
            "default": 8,
            "description": "LoRA attention dimension (rank).",
            "title": "R",
            "type": "integer"
        },
        "rank_pattern": {
            "default": {},
            "description": "Mapping of layer names/patterns to custom ranks different from default r.",
            "title": "Rank Pattern",
            "type": "object"
        },
        "target_modules": {
            "anyOf": [
                {
                    "items": {
                        "type": "string"
                    },
                    "type": "array"
                },
                {
                    "type": "string"
                },
                {
                    "type": "null"
                }
            ],
            "default": null,
            "description": "Names of modules to apply LoRA to. Can be a list of module names, a regex pattern, or 'all-linear'.",
            "title": "Target Modules"
        },
        "task_type": {
            "anyOf": [
                {
                    "type": "string"
                },
                {
                    "type": "null"
                }
            ],
            "default": null,
            "description": "Type of task being performed.",
            "title": "Task Type"
        },
        "use_rslora": {
            "default": false,
            "description": "Whether to use Rank-Stabilized LoRA which sets adapter scaling to lora_alpha/sqrt(r).",
            "title": "Use Rslora",
            "type": "boolean"
        }
    },
    "title": "LoraConfig",
    "type": "object"
}